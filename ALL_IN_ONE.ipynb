{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ALL IN ONE",
      "provenance": [],
      "authorship_tag": "ABX9TyM5qYcAaSwGI/fFpq2/H8M1",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/undefinedzack/stock-market-prediction-using-sentiment-analysis/blob/master/ALL_IN_ONE.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y72b9oKwnv6v",
        "outputId": "a32cae55-4cc4-4940-fbe7-3561973ac70a"
      },
      "source": [
        "# Data Manipulation\n",
        "\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import re\n",
        "\n",
        "# Preprocessing the input data\n",
        "\n",
        "import nltk\n",
        "from bs4 import BeautifulSoup\n",
        "from nltk import word_tokenize\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.stem.porter import PorterStemmer\n",
        "\n",
        "# Creating ngrams and vectorizing the data\n",
        "\n",
        "from gensim.models import Word2Vec, Phrases\n",
        "from gensim.models.phrases import Phraser\n",
        "\n",
        "# Tools for building a model\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from keras.models import Sequential\n",
        "from keras.layers import Dense, LSTM, Dropout, Bidirectional\n",
        "from keras.layers.embeddings import Embedding\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "import tensorflow as tf\n",
        "\n",
        "nltk.download('stopwords')\n",
        "nltk.download('punkt')\n",
        "nltk.download('wordnet')"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qvc-CeRwn3X2",
        "outputId": "4f53ee1b-ba99-44b9-fc46-0ef2ed05fa6e"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')\n",
        "%cd drive/MyDrive/Colab_Data/\n",
        "%ls -l"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n",
            "/content/drive/MyDrive/Colab_Data\n",
            "total 43488\n",
            "-rw------- 1 root root   965379 Feb 20 04:29 df_final.csv\n",
            "-rw------- 1 root root 34251771 Feb 15 11:03 df_stocktwits_prepared_final.csv\n",
            "drwx------ 2 root root     4096 Mar 31 08:18 \u001b[0m\u001b[01;34msaved_model\u001b[0m/\n",
            "-rw------- 1 root root   479968 Mar 19 13:58 stock_data.csv\n",
            "-rw------- 1 root root  7076794 Feb 18 05:11 stockerbot-export1.csv\n",
            "-rw------- 1 root root  1752624 Feb 18 05:11 tweet_sentiment.csv\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sgQfa5mRn65k"
      },
      "source": [
        "# creating dataframe\n",
        "#df1 = pd.read_csv('stock_data.csv')\n",
        "# df1 = pd.read_csv('stockerbot-export1.csv')\n",
        "\n",
        "df1 = pd.read_csv('tweet_sentiment.csv')\n",
        "# df = pd.read_csv('tweets_labelled.csv', error_bad_lines=False)"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 206
        },
        "id": "pO59fYkaoB2c",
        "outputId": "d327b22e-8d85-46c0-c480-c94207b49cef"
      },
      "source": [
        "text = []\n",
        "sentiment = []\n",
        "\n",
        "for i in range(len(df1)):\n",
        "  text.append(df1['cleaned_tweets'][i])\n",
        "  sentiment.append(df1['sentiment'][i])\n",
        "\n",
        "df = pd.DataFrame()                     # create Data Frame\n",
        "df['text'] = np.array(text)\n",
        "df['sentiment'] = np.array(sentiment)\n",
        "\n",
        "df.head(5)"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>text</th>\n",
              "      <th>sentiment</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>video offic mind busi david solomon tell gs in...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>price lumber lb f sinc hit ytd high maci turna...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>say american dream dead</td>\n",
              "      <td>-1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>barri silbert extrem optimist bitcoin predict ...</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>satellit avoid attack space junk circl earth paid</td>\n",
              "      <td>-1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                text  sentiment\n",
              "0  video offic mind busi david solomon tell gs in...          0\n",
              "1  price lumber lb f sinc hit ytd high maci turna...          0\n",
              "2                            say american dream dead         -1\n",
              "3  barri silbert extrem optimist bitcoin predict ...          1\n",
              "4  satellit avoid attack space junk circl earth paid         -1"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FAt-gtnToFKN"
      },
      "source": [
        "###### CLEANING EACH STRING\n",
        "def clean(tweet :str) -> str:\n",
        "  pat1= r'@[A-Za-z0-9]+'\n",
        "  pat2= r'https?://[A-Za-z0-9./]+'\n",
        "  combined_pat=r'|'.join((pat1,pat2))\n",
        "  pat3= r'[^a-zA-Z]'\n",
        "  combined_pat2=r'|'.join((combined_pat,pat3))\n",
        "  \n",
        "  # removing HTML\n",
        "  text = BeautifulSoup(tweet, \"lxml\").get_text()\n",
        "\n",
        "  # remove non-letters\n",
        "  letters_only = re.sub(combined_pat2, \" \", text)\n",
        "\n",
        "  # converting to lower-case\n",
        "  lowercase_letters = letters_only.lower()\n",
        "\n",
        "  return lowercase_letters\n",
        "\n",
        "\n",
        "##### LEMMATIZATION\n",
        "def lemmatize(tokens :list) -> list:\n",
        "  lemmatizer = WordNetLemmatizer()\n",
        "  ps= PorterStemmer()\n",
        "  stop_words = set(stopwords.words(\"english\"))  \n",
        " \n",
        "  # lemmatize\n",
        "  lemmatized_tokens = list(map(lemmatizer.lemmatize, tokens))\n",
        "\n",
        "  # remove stop words\n",
        "  meaningful_words = list(filter(lambda x : x not in stop_words, lemmatized_tokens))\n",
        "\n",
        "  tweets = [ps.stem(word) for word in meaningful_words]\n",
        "  return tweets\n",
        "\n",
        "\n",
        "###### ALL TOGETHER\n",
        "def preprocess(tweet :str) -> list:\n",
        "\n",
        "  # clean tweet\n",
        "  clean_tweet = clean(tweet)\n",
        "\n",
        "  # tokenize\n",
        "  tokens = word_tokenize(clean_tweet)\n",
        "\n",
        "  # lemmatize\n",
        "  lemmaz = lemmatize(tokens)\n",
        "\n",
        "  return lemmaz\n",
        "\n",
        "\n",
        "###### CLEANING WHOLE DATA BY PROCESSING EACH TWEET ONE BY ONE\n",
        "def get_clean_data(tweets):\n",
        "  return np.array(list(map(preprocess, tweets )))\n",
        "\n",
        "  \n",
        "###### BUILDING TRIGRAMS MODEL\n",
        "def build_trigrams_model(cleaned_data):\n",
        "  #creating n grams\n",
        "  bigrams = Phrases(sentences=cleaned_data)\n",
        "  trigrams = Phrases(sentences=bigrams[cleaned_data])\n",
        "  \n",
        "  # creating trigram model\n",
        "  embedding_vector_size = 256\n",
        "  trigrams_model = Word2Vec(\n",
        "      sentences = trigrams[bigrams[cleaned_data]],\n",
        "      size = embedding_vector_size,\n",
        "      min_count=3, window=5, workers=4)\n",
        "  \n",
        "  return trigrams_model\n",
        "\n",
        "\n",
        "###### VECTORIZING DATA\n",
        "def vectorize_data(data, vocab: dict) -> list:\n",
        "    print('Vectorize sentences...')\n",
        "    keys = list(vocab.keys())\n",
        "    filter_unknown = lambda word: vocab.get(word, None) is not None\n",
        "    encode = lambda tweet: list(map(keys.index, filter(filter_unknown, tweet)))\n",
        "    vectorized = list(map(encode, data))\n",
        "    print('Vectorize sentences... (done)')\n",
        "    return vectorized\n",
        "\n",
        "\n",
        "###### FINAL DATA WITH PADDING\n",
        "def vectorised_padded_data(cleaned_data):\n",
        "  \n",
        "  bigrams = Phrases(sentences=cleaned_data)\n",
        "  trigrams = Phrases(sentences=bigrams[cleaned_data])\n",
        "  X_data = trigrams[bigrams[cleaned_data]]\n",
        "  \n",
        "  print('Convert sentences to sentences with ngrams... (done)')\n",
        "  input_length = 150\n",
        "  \n",
        "  trigrams_model = build_trigrams_model(cleaned_data)\n",
        "  X_pad = pad_sequences(\n",
        "      sequences=vectorize_data(X_data, vocab=trigrams_model.wv.vocab),\n",
        "      maxlen=input_length,\n",
        "      padding='post')\n",
        "  return X_pad\n",
        "\n",
        "\n",
        "###### CLUBBING VECTORIZATION AND PADDING FUCTION\n",
        "def suitable_data(tweets):\n",
        "  cleaned_data = get_clean_data(tweets)\n",
        "  return vectorised_padded_data(cleaned_data)\n",
        "  \n",
        "\n"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UISdZXG6ravk",
        "outputId": "82a0c2f6-ab1a-4de9-91dd-69e76abf3697"
      },
      "source": [
        "X_pad = suitable_data(df['text'])"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.7/dist-packages/ipykernel_launcher.py:54: VisibleDeprecationWarning: Creating an ndarray from ragged nested sequences (which is a list-or-tuple of lists-or-tuples-or ndarrays with different lengths or shapes) is deprecated. If you meant to do this, you must specify 'dtype=object' when creating the ndarray\n",
            "/usr/local/lib/python3.7/dist-packages/gensim/models/phrases.py:598: UserWarning: For a faster implementation, use the gensim.models.phrases.Phraser class\n",
            "  warnings.warn(\"For a faster implementation, use the gensim.models.phrases.Phraser class\")\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "Convert sentences to sentences with ngrams... (done)\n",
            "Vectorize sentences...\n",
            "Vectorize sentences... (done)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bMOrxH6Ds44N"
      },
      "source": [
        "my_model = tf.keras.models.load_model('saved_model')"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "P6L_Wty1tKzK"
      },
      "source": [
        "outputs=my_model.predict(x=X_pad)"
      ],
      "execution_count": 12,
      "outputs": []
    }
  ]
}